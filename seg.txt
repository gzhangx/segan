https://github.com/gzhangx/segment-anything/blob/main/segment_anything/modeling/sam.py


sam forward:
  image_embeddings = ImageEncoderViT(preprocessing(image))
  for image_record, curr_embedding in zip(batched_input, image_embeddings):  
     PromptEncoder
     MaskDecoder
     postprocess_masks          
     

PromptEncoder:
    forward:
        self.mask_downscaling = nn.Sequential(
            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),
            LayerNorm2d(mask_in_chans // 4),
            activation(), = GELU
            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),
            LayerNorm2d(mask_in_chans),
            activation(),
            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),)



https://viso.ai/deep-learning/vision-transformer-vit/
https://arxiv.org/pdf/2010.11929.pdf
https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853
https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py



amg.py=>
    from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
    generator = SamAutomaticMaskGenerator(sam, output_mode=output_mode, **amg_kwargs)
    masks = generator.generate(image)


batch_iterator(self.points_per_batch, points_for_image):
self.points_per_batch 64 points_for_image [[ 10.         6.671875]
 [ 30.         6.671875]
 [ 50.         6.671875]
 ...
 [590.       420.328125]
 [610.       420.328125]
 [630.       420.328125]] points_scale [[640 427]]



 points in process_batch:
 [[ 10.       406.984375]
 [ 30.       406.984375]
 [ 50.       406.984375]
 [ 70.       406.984375]
 [ 90.       406.984375]
 [110.       406.984375]
 [130.       406.984375]
 [150.       406.984375]
 ] im_size (427, 640)


-> _init_.py
    from .automatic_mask_generator import SamAutomaticMaskGenerator
-> automatic_mask_generator.py
    SamAutomaticMaskGenerator
        def generate(self, image: np.ndarray) -> List[Dict[str, Any]]:
            mask_data = self._generate_masks(image)
        _generate_masks
            # orig_size (427, 640) (427, 640, 3)
            # crop_boxes, layer_idxs [[0, 0, 640, 427]] [0]
            # processing  [0, 0, 640, 427], 0, (427, 640)
            self._process_crop(image, crop_box, layer_idx, orig_size)
            ... remove duplicates ...
        def _process_crop(self,image: np.ndarray,crop_box: List[int],crop_layer_idx: int,orig_size: Tuple[int, ...],) -> MaskData:
            ... a lot of stuff ...
            ... batch iterator of points
            # self.points_per_batch, points_for_image 64 1024 64
            _process_batch
        _process_batch (self, points -> xy size of points to be processed)
            self.predictor.predict_torch, predictor is SamPredictor
            and a lot of stuff.

            

predictor.py
    SamPredictor
        model=>from segment_anything.modeling import Sam
        def predict(
            self,
            point_coords: Optional[np.ndarray] = None,
            point_labels: Optional[np.ndarray] = None,
            box: Optional[np.ndarray] = None,
            mask_input: Optional[np.ndarray] = None,
            multimask_output: bool = True,
            return_logits: bool = False,
            ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
            predict_torch
        def predict_torch
            sparse_embeddings, dense_embeddings = self.model.prompt_encoder(points, boxes, masks)
            low_res_masks, iou_predictions = self.model.mask_decoder (emddings from above)
            mask = self.model.postprocess_masks(low_res_masks)
            return maskes,iou_predictions, low_res_masks