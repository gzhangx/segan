https://github.com/gzhangx/segment-anything/blob/main/segment_anything/modeling/sam.py


sam forward:
  image_embeddings = ImageEncoderViT(preprocessing(image))
  for image_record, curr_embedding in zip(batched_input, image_embeddings):  
     PromptEncoder
     MaskDecoder
     postprocess_masks          
     

PromptEncoder:
    forward:
        self.mask_downscaling = nn.Sequential(
            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),
            LayerNorm2d(mask_in_chans // 4),
            activation(), = GELU
            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),
            LayerNorm2d(mask_in_chans),
            activation(),
            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),)



https://viso.ai/deep-learning/vision-transformer-vit/
https://arxiv.org/pdf/2010.11929.pdf
https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853
https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py



amg.py=>
    from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
    generator = SamAutomaticMaskGenerator(sam, output_mode=output_mode, **amg_kwargs)
    masks = generator.generate(image)


batch_iterator(self.points_per_batch, points_for_image):
self.points_per_batch 64 points_for_image [[ 10.         6.671875]
 [ 30.         6.671875]
 [ 50.         6.671875]
 ...
 [590.       420.328125]
 [610.       420.328125]
 [630.       420.328125]] points_scale [[640 427]]



 points in process_batch:
 [[ 10.       406.984375]
 [ 30.       406.984375]
 [ 50.       406.984375]
 [ 70.       406.984375]
 [ 90.       406.984375]
 [110.       406.984375]
 [130.       406.984375]
 [150.       406.984375]
 ] im_size (427, 640)


-> _init_.py
    from .automatic_mask_generator import SamAutomaticMaskGenerator
-> automatic_mask_generator.py
    SamAutomaticMaskGenerator
        def generate(self, image: np.ndarray) -> List[Dict[str, Any]]:
            mask_data = self._generate_masks(image)
        _generate_masks
            # orig_size (427, 640) (427, 640, 3)
            # crop_boxes, layer_idxs [[0, 0, 640, 427]] [0]
            # processing  [0, 0, 640, 427], 0, (427, 640)
            self._process_crop(image, crop_box, layer_idx, orig_size)
            ... remove duplicates ...
        def _process_crop(self,image: np.ndarray,crop_box: List[int],crop_layer_idx: int,orig_size: Tuple[int, ...],) -> MaskData:
            ... a lot of stuff ...
            ... batch iterator of points
            # self.points_per_batch, points_for_image 64 1024 64
            _process_batch
        _process_batch (self, points -> xy size of points to be processed)
            self.predictor.predict_torch, predictor is SamPredictor
            and a lot of stuff.

            

predictor.py
    SamPredictor
        model=>from segment_anything.modeling import Sam
        def predict(
            self,
            point_coords: Optional[np.ndarray] = None,
            point_labels: Optional[np.ndarray] = None,
            box: Optional[np.ndarray] = None,
            mask_input: Optional[np.ndarray] = None,
            multimask_output: bool = True,
            return_logits: bool = False,
            ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
            predict_torch
        def predict_torch
            sparse_embeddings, dense_embeddings = self.model.prompt_encoder(points, boxes, masks)
            low_res_masks, iou_predictions = self.model.mask_decoder (emddings from above, input has feature which is the imae feature)
            mask = self.model.postprocess_masks(low_res_masks)
            return maskes,iou_predictions, low_res_masks


        in predictor.pyinput_image = self.transform.apply_image(image)
        input_image_torch = torch.as_tensor(input_image, device=self.device)
        input_image_torch = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]

        set_tource_image:
            input_image = self.model.preprocess(transformed_image)
            self.features = self.model.image_encoder(input_image) image_encoder is ImageEncoderViT
amg.py:
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    generator = SamAutomaticMaskGenerator(sam, output_mode=output_mode, **amg_kwargs)
    masks = generator.generate(image)



sparse_embeddings tensor([[[-1.1913,  0.1880, -1.0566,  ..., -1.0741, -0.8410,  0.8908],
         [-0.0360,  0.0032, -0.0437,  ..., -0.1503, -0.1178, -0.1336]],
])

dense_embeddings tensor([[[[-0.0215, -0.0215, -0.0215,  ..., -0.0215, -0.0215, -0.0215],
          [-0.0215, -0.0215, -0.0215,  ..., -0.0215, -0.0215, -0.0215],
          [-0.0215, -0.0215, -0.0215,  ..., -0.0215, -0.0215, -0.0215],
          ]]])

self.features tensor([[[[ 1.1643e-01, -2.0583e-01, -9.9784e-02,  ..., -1.9412e-01,
            6.6644e-03, -7.2308e-02],
          [-1.8789e-01, -2.3789e-03,  9.0807e-02,  ...,  1.9272e-01,
           -9.7413e-04,  8.4644e-02],
          [-6.6448e-02, -3.6837e-02, -1.4350e-01,  ...,  1.0283e-01,
            2.8642e-02,  4.9216e-02],
]]])

mask shape before slice torch.Size([64, 4, 256, 256]) iou_pred torch.Size([64, 4])
mask shape torch.Size([64, 3, 256, 256]) iou_pred torch.Size([64, 3])




def predict_masks(
        self,
        image_embeddings: torch.Tensor,
        image_pe: torch.Tensor,
        sparse_prompt_embeddings: torch.Tensor,
        dense_prompt_embeddings: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Predicts masks. See 'forward' for more details."""
        # Concatenate output tokens
        output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)
        output_tokens = output_tokens.unsqueeze(0).expand(sparse_prompt_embeddings.size(0), -1, -1)
        tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=1)

        # Expand per-image data in batch direction to be per-mask
        src = torch.repeat_interleave(image_embeddings, tokens.shape[0], dim=0)
        src = src + dense_prompt_embeddings
        pos_src = torch.repeat_interleave(image_pe, tokens.shape[0], dim=0)
        b, c, h, w = src.shape

        # Run the transformer
        hs, src = self.transformer(src, pos_src, tokens)


in build_sam.py
transformer: TwoWayTransformer in transformer.py

TwoWayTransformer
    TwoWayAttentionBlock